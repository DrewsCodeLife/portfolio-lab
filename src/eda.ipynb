{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed4793a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8641e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cell, ensure this runs first!\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ac83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/used/\"\n",
    "\n",
    "# Loading the kaggle data\n",
    "KAGGLE_SUFFIX = \".us.txt\"   # All the kaggle-gathered data ends with this file type, convenience variable.\n",
    "BIL  = pd.read_csv(os.path.join(DATA_PATH, \"bil\" + KAGGLE_SUFFIX), parse_dates=[\"Date\"])\n",
    "BND  = pd.read_csv(os.path.join(DATA_PATH, \"bnd\" + KAGGLE_SUFFIX), parse_dates=[\"Date\"])\n",
    "EFA  = pd.read_csv(os.path.join(DATA_PATH, \"efa\" + KAGGLE_SUFFIX), parse_dates=[\"Date\"])\n",
    "RWR  = pd.read_csv(os.path.join(DATA_PATH, \"rwr\" + KAGGLE_SUFFIX), parse_dates=[\"Date\"])\n",
    "SPY  = pd.read_csv(os.path.join(DATA_PATH, \"spy\" + KAGGLE_SUFFIX), parse_dates=[\"Date\"])\n",
    "VNQ  = pd.read_csv(os.path.join(DATA_PATH, \"vnq\" + KAGGLE_SUFFIX), parse_dates=[\"Date\"])\n",
    "VTI  = pd.read_csv(os.path.join(DATA_PATH, \"vti\" + KAGGLE_SUFFIX), parse_dates=[\"Date\"])\n",
    "VXUS = pd.read_csv(os.path.join(DATA_PATH, \"vxus\" + KAGGLE_SUFFIX), parse_dates=[\"Date\"]) # Kaggle data only has ~1700 rows\n",
    "\n",
    "# ============== Loading other data sources ==============\n",
    "# From Federal Reserve Bank of St. Louis, https://fred.stlouisfed.org/series/DTB3\n",
    "DTB3 = pd.read_csv(os.path.join(DATA_PATH, \"DTB3.csv\"), parse_dates=[\"observation_date\"])\n",
    "DTB3.rename(columns={\"observation_date\": \"Date\"}, inplace=True)\n",
    "\n",
    "# From fetch_yahoo.py script\n",
    "# yfinance data has odd formatting, need to reformat before parsing date column.\n",
    "VBMFX = pd.read_csv(os.path.join(DATA_PATH, \"vbmfx.csv\"), header=None)\n",
    "\n",
    "col_names = VBMFX.iloc[0].tolist()\n",
    "\n",
    "col_names[0] = \"Date\"\n",
    "tckr = VBMFX.iloc[1, 1]\n",
    "\n",
    "VBMFX = VBMFX.iloc[3:].copy()\n",
    "VBMFX.columns = col_names\n",
    "\n",
    "VBMFX[\"Date\"] = pd.to_datetime(VBMFX[\"Date\"])\n",
    "\n",
    "BIL.set_index(\"Date\", inplace=True)\n",
    "BND.set_index(\"Date\", inplace=True)\n",
    "DTB3.set_index(\"Date\", inplace=True)\n",
    "EFA.set_index(\"Date\", inplace=True)\n",
    "RWR.set_index(\"Date\", inplace=True)\n",
    "SPY.set_index(\"Date\", inplace=True)\n",
    "VBMFX.set_index(\"Date\", inplace=True)\n",
    "VNQ.set_index(\"Date\", inplace=True)\n",
    "VTI.set_index(\"Date\", inplace=True)\n",
    "VXUS.set_index(\"Date\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3f38c",
   "metadata": {},
   "source": [
    "After some initial exploration, I found that some of the kaggle data had relatively few observations (~1700) while other data points had many observations (>3000). For now, I will continue exploring the data set as is, because my yahoo data is rate-limited, but long term I would like to pull down more data from yahoo finance, contrast it with my initial dataset, and potentially centralize my dataset around yahoo finance, for a longer horizon. For now, my Markowitz Optimization model will be highly tuned towards modern economic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d573b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NA in BIL  : Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "OpenInt    0\n",
      "dtype: int64\n",
      "Total NA in BND  : Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "OpenInt    0\n",
      "dtype: int64\n",
      "Total NA in DTB3 : DTB3    795\n",
      "dtype: int64\n",
      "Total NA in EFA  : Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "OpenInt    0\n",
      "dtype: int64\n",
      "Total NA in RWR  : Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "OpenInt    0\n",
      "dtype: int64\n",
      "Total NA in SPY  : Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "OpenInt    0\n",
      "dtype: int64\n",
      "Total NA in VBMFX: Close     1\n",
      "High      1\n",
      "Low       1\n",
      "Open      1\n",
      "Volume    0\n",
      "dtype: int64\n",
      "Total NA in VNQ  : Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "OpenInt    0\n",
      "dtype: int64\n",
      "Total NA in VTI  : Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "OpenInt    0\n",
      "dtype: int64\n",
      "Total NA in VXUS : Open       0\n",
      "High       0\n",
      "Low        0\n",
      "Close      0\n",
      "Volume     0\n",
      "OpenInt    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total NA in BIL  : {BIL.isna().sum()}\")\n",
    "print(f\"Total NA in BND  : {BND.isna().sum()}\")\n",
    "print(f\"Total NA in DTB3 : {DTB3.isna().sum()}\")\n",
    "print(f\"Total NA in EFA  : {EFA.isna().sum()}\")\n",
    "print(f\"Total NA in RWR  : {RWR.isna().sum()}\")\n",
    "print(f\"Total NA in SPY  : {SPY.isna().sum()}\")\n",
    "print(f\"Total NA in VBMFX: {VBMFX.isna().sum()}\")\n",
    "print(f\"Total NA in VNQ  : {VNQ.isna().sum()}\")\n",
    "print(f\"Total NA in VTI  : {VTI.isna().sum()}\")\n",
    "print(f\"Total NA in VXUS : {VXUS.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d584a4",
   "metadata": {},
   "source": [
    "Only the DTB3 data set has missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf9e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drewm\\AppData\\Local\\Temp\\ipykernel_5592\\1920781367.py:10: Pandas4Warning: Sorting by default when concatenating all DatetimeIndex is deprecated.  In the future, pandas will respect the default of `sort=False`. Specify `sort=True` or `sort=False` to silence this message. If you see this warnings when not directly calling concat, report a bug to pandas.\n",
      "  combined = pd.concat(close_vals, axis=1)\n"
     ]
    }
   ],
   "source": [
    "close_vals =  [\n",
    "                BIL[\"Close\"]  , BND[\"Close\"],\n",
    "                DTB3[\"DTB3\"]  , EFA[\"Close\"],\n",
    "                RWR[\"Close\"]  , SPY[\"Close\"],\n",
    "                VBMFX[\"Close\"], VNQ[\"Close\"],\n",
    "                VTI[\"Close\"]  , VXUS[\"Close\"],\n",
    "              ]\n",
    "\n",
    "col_names = [\"BIL\", \"BND\", \"DTB3\", \"EFA\", \"RWR\", \"SPY\", \"VBMFX\", \"VNQ\", \"VTI\", \"VXUS\"]\n",
    "combined = pd.concat(close_vals, axis=1)\n",
    "combined.columns = col_names\n",
    "\n",
    "combined.sort_index(inplace=True)\n",
    "cleaned = combined.dropna(how=\"any\")  # Find the max period where no feature has an NA\n",
    "\n",
    "cleaned[col_names] = cleaned[col_names].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44de05",
   "metadata": {},
   "source": [
    "It looks like our dataset has a complete and consistent daily close price for each of our selected stocks, for a period stretching from  2011-02-01 to 2017-11-10, roughly 6.5 years. My goal of Markowitz Optimization can be significantly skewed by time, policies such as ZIRP can significantly change the relationships between features studied in this project, altering the mathematical optimum. As such, it is ideal for this project to collect further data enabling a window closer to the modern day, but this is sufficient for now.\n",
    "\n",
    "`VXUS`, `BIL`, and `BND` have the smallest number of observations (`VXUS` ~1700, `BIL/BND` ~2600)\n",
    "\n",
    "The `cleaned` dataframe now contains date-aligned, frequency-matched, gapless data for all of the features used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d9f0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\drewm\\miniforge3\\envs\\capstone\\Lib\\site-packages\\numpy\\_core\\_methods.py:49: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "ret = cleaned.iloc[1:].pct_change() # Simple daily returns\n",
    "\n",
    "means = []\n",
    "for idx, col in enumerate(list(ret.columns)):\n",
    "  means.append(ret[col].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6d96b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
